# UAV_USV_RL_V6

 **基于强化学习的无人机-无人船多智能体协同作战仿真系统**

##  一、项目简介

本项目是一个基于深度强化学习的多智能体协同作战仿真系统，模拟红蓝双方无人机(UAV)和无人船(USV)的对抗场景。系统采用PPO(Proximal Policy Optimization)算法训练智能体，实现端到端的路径规划、目标分配和火力控制。

###  核心特性

- **多智能体协同**: 支持UAV和USV混合编队作战
- **端到端学习**: 集成路径规划、目标分配、火力分配的统一决策
- **3D战场环境**: 完整的三维作战空间模拟
- **实时可视化**: 基于Web的3D战场态势显示
- **轨迹记录**: 完整的训练和测试轨迹数据记录

##  二、项目架构

```
UAV_USV_RL_V6/
├── src/                          # 核心源代码
│   ├── agents/                   # 智能体定义
│   │   └── agent.py             # Agent和Base类实现
│   ├── env/                      # 环境模块
│   │   ├── config.py            # 环境配置参数
│   │   └── world.py             # 战场环境实现
│   ├── ppo/                      # PPO算法实现
│   │   ├── model.py             # 神经网络模型
│   │   ├── ppo.py               # PPO训练算法
│   │   └── obsDataProcess.py    # 观测数据处理
│   └── utils/                    # 工具模块
│       ├── trajectory_manager.py # 轨迹记录管理
│       └── plot_kl_rew.py       # 训练曲线绘制
├── scripts/                      # 脚本文件
│   ├── train.py                 # 训练脚本
│   ├── visualize.html           # 3D可视化界面
│   ├── weights/                 # 模型权重存储
│   ├── log/                     # 训练日志
│   └── trajectory_records/      # 轨迹记录文件
└── README.md                     # 项目说明文档
```

## 三、🚀 快速开始

### 环境要求

- Python 3.9+
- PyTorch 2.7.0
- NumPy
- matplotlib
- typing 4.10.0+
- 支持CUDA的GPU

### 安装依赖

```bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
pip install numpy 
pip install matplotlib
```

### 训练模型

```bash
cd scripts
python train.py
```

### 可视化结果

1. 在浏览器中打开 `scripts/visualize.html`
2. 加载轨迹记录文件
3. 观看3D战场回放

## 四、⚙️ 配置说明

### 智能体配置

| 类型 | 数量 | HP | 火力 | 速度 | 攻击范围 | 探测范围 |
|------|------|----|----|------|----------|----------|
| 红方UAV | 15 | 1 | 1 | 10 | 2 | 20 |
| 红方USV | 5 | 3 | 3 | 5 | 80 | 130 |
| 蓝方UAV | 12 | 1 | 1 | 10 | 2 | 20 |
| 蓝方USV | 3 | 3 | 3 | 5 | 80 | 130 |

### 战场环境

- **世界边界**: X[-50, 250], Y[-50, 250], Z[0, 50]
- **最大步数**: 200步
- **蓝方基地**: 位置[201, 201, 0], HP=1

### 奖励机制

#### 基础奖励
- **红方摧毁蓝方基地**: +180.0
- **蓝方防守成功**: -150.0 (红方未能摧毁基地时的惩罚)
- **越界惩罚**: -0.05 (智能体超出战场边界)
- **攻击成功奖励**: +0.5 (成功摧毁敌方智能体)

#### 距离奖励机制
- **红方距离奖励**: 最大+0.3
  - 距离蓝方基地30米内：获得最大奖励0.3
  - 在正确方向前进：根据进度线性递减(0.3 × 进度比例)
  - 越过基地或背离方向：无奖励
  - 采用方向感知算法，防止智能体"穿越"基地行为

#### 奖励设计特点
- **方向导向**: 红方距离奖励基于投影算法，确保智能体朝向目标前进
- **防穿越机制**: 智能体越过基地后不再获得距离奖励
- **平衡性**: 攻击奖励与防守惩罚比例合理，促进积极对抗
- **边界约束**: 越界惩罚较小(-0.05)，避免过度限制探索

## 五、算法特点

### PPO强化学习

- **策略网络**: MLP架构，支持连续动作空间
- **价值网络**: 独立的价值函数估计
- **观测编码**: 基于Transformer的多实体观测处理
- **目标分配**: 离散动作空间的攻击目标分配和攻击动作决策
- **优势函数**: 基于GAE的优势函数估计

### 多智能体协调

- **分布式训练**: 每个智能体独立决策
- **局部观测**: 基于探测范围的部分可观测环境
- **通信机制**: 友方智能体信息共享

##  六、训练监控

系统提供完整的训练监控功能：

- **实时日志**: 详细的训练过程记录
- **奖励曲线**: 训练奖励和KL散度可视化
- **轨迹记录**: 每个episode的完整轨迹数据
- **模型保存**: 自动保存最佳模型权重

##  七、可视化功能

### 3D战场显示

- **实体渲染**: 不同颜色区分红蓝双方
- **轨迹追踪**: 智能体移动路径显示
- **状态信息**: 实时HP、ID等状态
- **回放控制**: 支持暂停、快进、倒退

### 数据分析

- **胜率统计**: 红蓝双方胜负比例
- **生存分析**: 智能体存活时间统计
- **效率评估**: 弹药使用效率分析

##  八、自定义配置

### 修改智能体数量

在 `src/env/config.py` 中修改

### 调整奖励函数

在 `src/env/world.py` 中修改奖励计算逻辑，在 `src/env/config.py` 中修改奖励权重